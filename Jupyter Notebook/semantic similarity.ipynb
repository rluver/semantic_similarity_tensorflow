{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Bidirectional, LSTM, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, Dropout, Dense\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertConfig, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorizer(label):\n",
    "    \n",
    "    if label == 'entailment':\n",
    "        return 2\n",
    "    elif label == 'neutral':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(sentence1, sentence2):\n",
    "    \n",
    "    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n",
    "    test_data = BertSemanticDataGenerator(\n",
    "        sentence_pairs, \n",
    "        labels = None, \n",
    "        batch_size = 1, \n",
    "        max_len = 128,\n",
    "        shuffle = False,\n",
    "        include_targets = False\n",
    "    )\n",
    "\n",
    "    proba = prediction_model.predict(test_data)[0]\n",
    "    idx = np.argmax(proba)\n",
    "    proba = f\"{(proba[idx] * 100): .2f}%\"\n",
    "    pred = labels[idx]\n",
    "    \n",
    "    return pred, proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_length):\n",
    "\n",
    "    input_ids = Input(\n",
    "        shape = (max_length, ), dtype = tf.int32, name = 'input_ids'\n",
    "    )\n",
    "    attention_masks = tf.keras.layers.Input(\n",
    "        shape = (max_length, ), dtype = tf.int32, name = 'attention_masks'\n",
    "    )\n",
    "    token_type_ids = tf.keras.layers.Input(\n",
    "        shape = (max_length, ), dtype = tf.int32, name = 'segment_ids'\n",
    "    )\n",
    "\n",
    "    # model load\n",
    "    config = BertConfig.from_pretrained(r'model', output_hidden_states = True)\n",
    "    bert_model = bert_model = TFBertModel.from_pretrained(r'model', from_pt = True, config = config)\n",
    "    # Freeze the BERT model to reuse the pretrained features without modifying them.\n",
    "    bert_model.trainable = False\n",
    "\n",
    "    outputs = bert_model([input_ids, token_type_ids, attention_masks])\n",
    "    sequence_output, _ = outputs.last_hidden_state, outputs.pooler_output\n",
    "    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n",
    "    bi_lstm = Bidirectional(\n",
    "        LSTM(units = 64, return_sequences = True)\n",
    "    )(sequence_output)\n",
    "    # Applying hybrid pooling approach to bi_lstm sequence output.\n",
    "    avg_pool = GlobalAveragePooling1D()(bi_lstm)\n",
    "    max_pool = GlobalMaxPooling1D()(bi_lstm)\n",
    "    concat = concatenate([avg_pool, max_pool])\n",
    "    dropout = Dropout(rate = 0.3)(concat)\n",
    "    output = Dense(units = 3, activation = 'softmax')(dropout)\n",
    "    model = Model(\n",
    "        inputs = [input_ids, token_type_ids, attention_masks], \n",
    "        outputs = output\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = Adam(),\n",
    "        loss = 'categorical_crossentropy',\n",
    "        metrics = ['accuracy'],\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSemanticDataGenerator(Sequence):\n",
    "    \"\"\"Generates batches of data.\n",
    "    Args:\n",
    "        sentence_pairs: Array of premise and hypothesis input sentences.\n",
    "        labels: Array of labels.\n",
    "        batch_size: Integer batch size.\n",
    "        shuffle: boolean, whether to shuffle the data.\n",
    "        include_targets: boolean, whether to incude the labels.\n",
    "    Returns:\n",
    "        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n",
    "        (or just `[input_ids, attention_mask, `token_type_ids]`\n",
    "         if `include_targets=False`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sentence_pairs,\n",
    "        labels,\n",
    "        batch_size,\n",
    "        shuffle = True,\n",
    "        include_targets = True,\n",
    "        max_len = 128\n",
    "    ):\n",
    "        self.sentence_pairs = sentence_pairs\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.include_targets = include_targets        \n",
    "        # Load our BERT Tokenizer to encode the text.\n",
    "        # We will use base-base-uncased pretrained model.\n",
    "        self.tokenizer = FullTokenizer('vocab.korean.rawtext.list')\n",
    "        self.max_len = max_len\n",
    "        self.indexes = np.arange(len(self.sentence_pairs))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch.\n",
    "        return len(self.sentence_pairs) // self.batch_size\n",
    "    \n",
    "    \n",
    "\n",
    "    def get_batch_bert_input_data(self, sentence_pairs):\n",
    "        \n",
    "    \n",
    "        sentence_pairs = list(map(lambda x: ' '.join(['[CLS]', x[0], '[SEP]', x[1], '[SEP]']), sentence_pairs))\n",
    "    \n",
    "        input_ids = map(lambda x: self.tokenizer.wordpiece_tokenizer.tokenize(x), sentence_pairs)\n",
    "        input_ids = list(map(lambda x: self.tokenizer.convert_tokens_to_ids(x), input_ids))\n",
    "                \n",
    "        mask_array = list(map(lambda x: [1] * len(x), input_ids))\n",
    "        input_mask_array = pad_sequences(mask_array, maxlen = self.max_len, padding = 'post')\n",
    "        \n",
    "        segment_index_lists = list(map(lambda x: np.where(x == tf.constant(3))[0], input_ids))\n",
    "        input_segment_array = list(map(lambda x: ( [0] * (x[0] + 1) ) + [1] * ( x[1] - x[0] ), segment_index_lists))\n",
    "        input_segment_array = pad_sequences(input_segment_array, maxlen = self.max_len, padding = 'post')\n",
    "        \n",
    "        input_id_array = pad_sequences(input_ids, maxlen = self.max_len, padding = 'post', dtype = 'int32')\n",
    "        \n",
    "        return [input_id_array, input_mask_array, input_segment_array]\n",
    "    \n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieves the batch of index.\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        sentence_pairs = self.sentence_pairs[indexes]\n",
    "\n",
    "        # Set to true if data generator is used for training/validation.\n",
    "        if self.include_targets:\n",
    "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
    "            return self.get_batch_bert_input_data(sentence_pairs), labels\n",
    "        else:\n",
    "            return self.get_batch_bert_input_data(sentence_pairs)\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle indexes after each epoch if shuffle is set to True.\n",
    "        if self.shuffle:\n",
    "            np.random.RandomState(42).shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 24426: expected 3 fields, saw 4\\nSkipping line 156343: expected 3 fields, saw 4\\nSkipping line 218766: expected 3 fields, saw 4\\nSkipping line 232318: expected 3 fields, saw 4\\nSkipping line 253493: expected 3 fields, saw 4\\n'\n",
      "b'Skipping line 265734: expected 3 fields, saw 4\\nSkipping line 282588: expected 3 fields, saw 4\\nSkipping line 350969: expected 3 fields, saw 4\\n'\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pd.read_csv(r'multinli.train.ko.tsv.txt', sep = '\\t', error_bad_lines = False).dropna().reset_index(drop = True)    \n",
    "train_dataset.gold_label = train_dataset.gold_label.apply(lambda x: categorizer(x))\n",
    "y_train = to_categorical(train_dataset.gold_label, num_classes = 3)\n",
    "train_data = BertSemanticDataGenerator(\n",
    "    train_dataset[['sentence1', 'sentence2']].values.astype('str'), \n",
    "    y_train, \n",
    "    batch_size = 512, \n",
    "    max_len = 128,\n",
    "    shuffle = True\n",
    "    )\n",
    "\n",
    "valid_dataset = pd.read_csv(r'xnli.test.ko.tsv.txt', sep = '\\t', error_bad_lines = False).dropna().reset_index(drop = True)\n",
    "valid_dataset.gold_label = valid_dataset.gold_label.apply(lambda x: categorizer(x))\n",
    "y_valid = to_categorical(valid_dataset.gold_label, num_classes = 3)\n",
    "valid_data = BertSemanticDataGenerator(\n",
    "    valid_dataset[['sentence1', 'sentence2']].values.astype('str'), \n",
    "    y_valid, \n",
    "    batch_size = 512, \n",
    "    max_len = 128,\n",
    "    shuffle = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min',\n",
    "    verbose = 1,\n",
    "    patience = 2\n",
    "    )\n",
    "\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir = 'log'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['contradiction', 'neutral', 'entailment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fee840f86c8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7fee8a149a58> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fee840f86c8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7fee8a149a58> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fee87862e18> and will run it as-is.\n",
      "Cause: while/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function wrap at 0x7fee87862e18> and will run it as-is.\n",
      "Cause: while/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_masks (InputLayer)    [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 109693440   input_ids[0][0]                  \n",
      "                                                                 segment_ids[0][0]                \n",
      "                                                                 attention_masks[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 128, 128)     426496      tf_bert_model[0][13]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 128)          0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 256)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            771         dropout_37[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 110,120,707\n",
      "Trainable params: 427,267\n",
      "Non-trainable params: 109,693,440\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "752/752 [==============================] - ETA: 0s - loss: 0.8970 - accuracy: 0.5818INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "752/752 [==============================] - 559s 743ms/step - loss: 0.8970 - accuracy: 0.5818 - val_loss: 0.7710 - val_accuracy: 0.6536\n",
      "Epoch 2/15\n",
      "752/752 [==============================] - 551s 733ms/step - loss: 0.7958 - accuracy: 0.6487 - val_loss: 0.6986 - val_accuracy: 0.7086\n",
      "Epoch 3/15\n",
      "752/752 [==============================] - 551s 732ms/step - loss: 0.7597 - accuracy: 0.6689 - val_loss: 0.6895 - val_accuracy: 0.7016\n",
      "Epoch 4/15\n",
      "752/752 [==============================] - 551s 732ms/step - loss: 0.7370 - accuracy: 0.6817 - val_loss: 0.6825 - val_accuracy: 0.7062\n",
      "Epoch 5/15\n",
      "752/752 [==============================] - 551s 732ms/step - loss: 0.7189 - accuracy: 0.6924 - val_loss: 0.6632 - val_accuracy: 0.7220\n",
      "Epoch 6/15\n",
      "752/752 [==============================] - 551s 732ms/step - loss: 0.7051 - accuracy: 0.6990 - val_loss: 0.6484 - val_accuracy: 0.7279\n",
      "Epoch 7/15\n",
      "752/752 [==============================] - 551s 732ms/step - loss: 0.6919 - accuracy: 0.7063 - val_loss: 0.6481 - val_accuracy: 0.7231\n",
      "Epoch 8/15\n",
      "752/752 [==============================] - 551s 732ms/step - loss: 0.6803 - accuracy: 0.7122 - val_loss: 0.6410 - val_accuracy: 0.7289\n",
      "Epoch 9/15\n",
      "752/752 [==============================] - 551s 732ms/step - loss: 0.6698 - accuracy: 0.7171 - val_loss: 0.6278 - val_accuracy: 0.7433\n",
      "Epoch 10/15\n",
      "752/752 [==============================] - 550s 732ms/step - loss: 0.6609 - accuracy: 0.7212 - val_loss: 0.6430 - val_accuracy: 0.7296\n",
      "Epoch 11/15\n",
      "752/752 [==============================] - 551s 732ms/step - loss: 0.6524 - accuracy: 0.7256 - val_loss: 0.6396 - val_accuracy: 0.7326\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "\n",
    "with strategy.scope():\n",
    "    model = build_model(max_length = 128)\n",
    "    # feature extraction\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        validation_data = valid_data,\n",
    "        epochs = EPOCHS,\n",
    "        use_multiprocessing = True,\n",
    "        workers = -1,\n",
    "        callbacks = [early_stopping]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = BertSemanticDataGenerator(\n",
    "    train_dataset[['sentence1', 'sentence2']].values.astype('str'), \n",
    "    y_train, \n",
    "    batch_size = 160, \n",
    "    max_len = 128,\n",
    "    shuffle = True\n",
    "    )\n",
    "\n",
    "valid_data = BertSemanticDataGenerator(\n",
    "    valid_dataset[['sentence1', 'sentence2']].values.astype('str'), \n",
    "    y_valid,\n",
    "    batch_size = 160,\n",
    "    max_len = 128,\n",
    "    shuffle = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_masks (InputLayer)    [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 109693440   input_ids[0][0]                  \n",
      "                                                                 segment_ids[0][0]                \n",
      "                                                                 attention_masks[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 128, 128)     426496      tf_bert_model[0][13]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 128)          0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 256)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            771         dropout_37[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 110,120,707\n",
      "Trainable params: 110,120,707\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... \n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:batch_all_reduce: 202 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:batch_all_reduce: 202 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "   1/2408 [..............................] - ETA: 0s - loss: 0.6403 - acc: 0.7375WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "2408/2408 [==============================] - ETA: 0s - loss: 0.6080 - acc: 0.7483WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "2408/2408 [==============================] - 1618s 672ms/step - loss: 0.6080 - acc: 0.7483 - val_loss: 0.5793 - val_acc: 0.7665\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2408/2408 [==============================] - 1607s 667ms/step - loss: 0.5491 - acc: 0.7771 - val_loss: 0.5714 - val_acc: 0.7694\n",
      "Epoch 3/50\n",
      "2408/2408 [==============================] - 1608s 668ms/step - loss: 0.5016 - acc: 0.8002 - val_loss: 0.5614 - val_acc: 0.7769\n",
      "Epoch 4/50\n",
      "2408/2408 [==============================] - 1608s 668ms/step - loss: 0.4573 - acc: 0.8193 - val_loss: 0.5678 - val_acc: 0.7763\n",
      "Epoch 5/50\n",
      "2408/2408 [==============================] - 1607s 667ms/step - loss: 0.4152 - acc: 0.8384 - val_loss: 0.5795 - val_acc: 0.7715\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "# fine-tuning\n",
    "model.trainable = True\n",
    "model.compile(\n",
    "    optimizer = Adam(1e-5),\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['acc']\n",
    "    )\n",
    "\n",
    "model.summary()\n",
    "with strategy.scope():\n",
    "    history = model.fit(        \n",
    "        train_data,\n",
    "        validation_data = valid_data,\n",
    "        epochs = 50,\n",
    "        use_multiprocessing = True,\n",
    "        workers = -1,\n",
    "        callbacks = [early_stopping, tensorboard]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('semantic_similarity/semantic_similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_masks (InputLayer)    [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model_1 (TFBertModel)   TFBaseModelOutputWit 109693440   input_ids[0][0]                  \n",
      "                                                                 segment_ids[0][0]                \n",
      "                                                                 attention_masks[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 128, 128)     426496      tf_bert_model_1[0][13]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)            (None, 256)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            771         dropout_75[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 110,120,707\n",
      "Trainable params: 427,267\n",
      "Non-trainable params: 109,693,440\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "prediction_model = build_model(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fcefa167e48>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_model.load_weights('semantic_similarity/semantic_similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('contradiction', ' 60.17%')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity('델타변이, 마스크도 돌파…대전 태권도장발 185명 집단감염', '27일부터 비수도권 거리두기 3단계로 격상…식당·카페 밤 10시까지(상보)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('contradiction', ' 65.06%')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity('백신 안 맞으면 미국 못간다.. 美, 입국하는 외국인에 접종 의무화 계획', '로이터 “미국 정부, 백신 접종완료 외국인만 입국허용 계획”')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('entailment', ' 90.35%')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity('영국 16∼17세 코로나19 백신접종 곧 시작…\"연령 확대 검토\"(종합)', '영국 16∼17세 코로나19 백신접종 곧 시작…\"연령 확대 검토\"(종합2보)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('entailment', ' 96.12%')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity('이낙연 “서울공항 이전해 주택 3만 채 공급…공공주도로 개발”', '이낙연 “서울공항 옮기고, 그 자리에 3만호”…주택 공급 공약')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('contradiction', ' 77.69%')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity('올게 왔다…도쿄올림픽 선수촌서 첫 집단 감염', '올림픽 선수촌 첫 집단감염…\"그리스 선수단 5명 양성\" [Tokyo 2020]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neutral', ' 80.51%')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity('올게 왔다…도쿄올림픽 선수촌서 첫 집단 감염', '올림픽 선수촌 첫 집단감염…\"그리스 선수단 5명 양성\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
